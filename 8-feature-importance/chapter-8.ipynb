{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a46ed3f6-01e2-499c-a7ed-01f2bc0375c9",
   "metadata": {},
   "source": [
    "# Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d144d02-5a45-4dd2-84b4-cdabd4b65bb2",
   "metadata": {},
   "source": [
    "### SNIPPET 8.2 MDI FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222b0af-18dd-4808-ba88-b55f50066618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featImpMDI(fit,featNames):\n",
    "    # feat importance based on IS mean impurity reduction\n",
    "    df0={i:tree.feature_importances_ for i,tree in enumerate(fit.estimators_)} \n",
    "    df0=pd.DataFrame.from_dict(df0,orient='index')\n",
    "    df0.columns=featNames\n",
    "    df0=df0.replace(0,np.nan) # because max_features=1\n",
    "    imp=pd.concat({'mean':df0.mean(),'std':df0.std()*df0.shape[0]**-.5},axis=1) \n",
    "    imp/=imp['mean'].sum()\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36949af8-e7e1-4541-ae27-8f01a7d382d2",
   "metadata": {},
   "source": [
    "### SNIPPET 8.3 MDA FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1ffbe-5564-44c4-a819-f53810b51f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score \n",
    "\n",
    "def featImpMDA(clf,X,y,cv,sample_weight,t1,pctEmbargo,scoring='neg_log_loss'): \n",
    "    '''    \n",
    "    feat importance based on OOS score reduction\n",
    "    '''    \n",
    "    if scoring not in ['neg_log_loss','accuracy']:\n",
    "        raise Exception('wrong scoring method.')\n",
    "    \n",
    "    cvGen=PurgedKFold(n_splits=cv,t1=t1,pctEmbargo=pctEmbargo) # purged cv \n",
    "    scr0,scr1=pd.Series(),pd.DataFrame(columns=X.columns)\n",
    "\n",
    "    for i,(train,test) in enumerate(cvGen.split(X=X)): \n",
    "        X0,y0,w0=X.iloc[train,:],y.iloc[train],sample_weight.iloc[train] \n",
    "        X1,y1,w1=X.iloc[test,:],y.iloc[test],sample_weight.iloc[test] \n",
    "        fit=clf.fit(X=X0,y=y0,sample_weight=w0.values)\n",
    "        if scoring=='neg_log_loss':\n",
    "            prob=fit.predict_proba(X1) \n",
    "            scr0.loc[i]=-log_loss(y1,prob,sample_weight=w1.values,labels=clf.classes_)\n",
    "        else:\n",
    "            pred=fit.predict(X1) \n",
    "            scr0.loc[i]=accuracy_score(y1,pred,sample_weight=w1.values)\n",
    "        for j in X.columns:\n",
    "            X1_=X1.copy(deep=True)\n",
    "            np.random.shuffle(X1_[j].values) # permutation of a single column \n",
    "            if scoring=='neg_log_loss':\n",
    "                prob=fit.predict_proba(X1_) \n",
    "                scr1.loc[i,j]=-log_loss(y1,prob,sample_weight=w1.values,labels=clf.classes_)\n",
    "            else:\n",
    "                pred=fit.predict(X1_) \n",
    "                scr1.loc[i,j]=accuracy_score(y1,pred,sample_weight=w1.values)\n",
    "    imp=(-scr1).add(scr0,axis=0)\n",
    "    if scoring=='neg_log_loss':\n",
    "        imp=imp/-scr1\n",
    "    else:\n",
    "        imp=imp/(1.-scr1) \n",
    "    imp=pd.concat({'mean':imp.mean(),'std':imp.std()*imp.shape[0]**-.5},axis=1) \n",
    "    return imp,scr0.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f00bfe-fe86-4267-b945-6e9df33b7475",
   "metadata": {},
   "source": [
    "### SNIPPET 8.4 IMPLEMENTATION OF SFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a909a6-85f3-4941-ac53-6cd72d900508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auxFeatImpSFI(featNames,clf,trnsX,cont,scoring,cvGen): \n",
    "    imp=pd.DataFrame(columns=['mean','std'])\n",
    "    for featName in featNames:\n",
    "        df0=cvScore(clf,X=trnsX[[featName]],y=cont['bin'],sample_weight=cont['w'], scoring=scoring,cvGen=cvGen)\n",
    "        imp.loc[featName,'mean']=df0.mean()\n",
    "        imp.loc[featName,'std']=df0.std()*df0.shape[0]**-.5 \n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67cbff6-00e4-4825-b7dc-e3b405fa5758",
   "metadata": {},
   "source": [
    "### SNIPPET 8.5 COMPUTATION OF ORTHOGONAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c21f7a-a06f-444b-8770-11dcdc492523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eVec(dot,varThres):\n",
    "    '''compute eVec from dot prod matrix, reduce dimension'''\n",
    "    eVal,eVec=np.linalg.eigh(dot)\n",
    "    idx=eVal.argsort()[::-1] # arguments for sorting eVal desc \n",
    "    eVal,eVec=eVal[idx],eVec[:,idx]\n",
    "    #2) only positive eVals\n",
    "    eVal=pd.Series(eVal,index=['PC_'+str(i+1) for i in range(eVal.shape[0])]) \n",
    "    eVec=pd.DataFrame(eVec,index=dot.index,columns=eVal.index) \n",
    "    eVec=eVec.loc[:,eVal.index]\n",
    "    #3) reduce dimension, form PCs\n",
    "    cumVar=eVal.cumsum()/eVal.sum()\n",
    "    dim=cumVar.values.searchsorted(varThres) \n",
    "    eVal,eVec=eVal.iloc[:dim+1],eVec.iloc[:,:dim+1]\n",
    "    return eVal,eVec\n",
    "\n",
    "def orthoFeats(dfX,varThres=.95):\n",
    "    '''Given a dataframe dfX of features, compute orthofeatures dfP '''\n",
    "    dfZ=dfX.sub(dfX.mean(),axis=1).div(dfX.std(),axis=1) # standardize \n",
    "    dot=pd.DataFrame(np.dot(dfZ.T,dfZ),index=dfX.columns,columns=dfX.columns) \n",
    "    eVal,eVec=get_eVec(dot,varThres)\n",
    "    dfP=np.dot(dfZ,eVec) \n",
    "    return dfP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c9489-eeda-4c40-873f-f433525d7c41",
   "metadata": {},
   "source": [
    "### SNIPPET 8.6 COMPUTATION OF WEIGHTED KENDALL’S TAU BETWEEN FEATURE IMPORTANCE AND INVERSE PCA RANKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c5684a-49d7-4676-9525-58dbb49a8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import weightedtau\n",
    "featImp=np.array([.55,.33,.07,.05]) # feature importance \n",
    "pcRank=np.array([1,2,4,3]) # PCA rank\n",
    "weightedtau(featImp,pcRank**-1.)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a032d-87f7-435c-babb-acc3ffb7f719",
   "metadata": {},
   "source": [
    "### SNIPPET 8.7 CREATING A SYNTHETIC DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbd117-2875-407b-9113-bc677681f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification \n",
    "\n",
    "def getTestData(n_features=40,n_informative=10,n_redundant=10,n_samples=10000): \n",
    "    '''generate a random dataset for a classification problem'''\n",
    "    trnsX,cont=make_classification(n_samples=n_samples,n_features=n_features,\n",
    "                                   n_informative=n_informative,n_redundant=n_redundant,random_state=0,\n",
    "                                   shuffle=False) \n",
    "    df0=pd.DatetimeIndex(periods=n_samples,freq=pd.tseries.offsets.BDay(),\n",
    "                         end=pd.datetime.today()) \n",
    "    trnsX,cont=pd.DataFrame(trnsX,index=df0),pd.Series(cont,index=df0).to_frame('bin') \n",
    "    df0=['I_'+str(i) for i in xrange(n_informative)]+['R_'+str(i) for i in xrange(n_redundant)] \n",
    "    df0+=['N_'+str(i) for i in xrange(n_features-len(df0))] \n",
    "    trnsX.columns=df0\n",
    "    cont['w']=1./cont.shape[0] \n",
    "    cont['t1']=pd.Series(cont.index,index=cont.index) \n",
    "    return trnsX,cont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed3bd1-55f7-4252-80b8-8660269b0695",
   "metadata": {},
   "source": [
    "### SNIPPET 8.8 CALLING FEATURE IMPORTANCE FOR ANY METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82aca6d-58c0-4ad4-adea-953eacda3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from mpEngine import mpPandasObj\n",
    "\n",
    "def featImportance(trnsX,cont,n_estimators=1000,cv=10,max_samples=1.,numThreads=24, pctEmbargo=0,scoring='accuracy',method='SFI',minWLeaf=0.,**kargs):\n",
    "    '''feature importance from a random forest'''\n",
    "    n_jobs=(-1 if numThreads>1 else 1) # run 1 thread with ht_helper in dirac1 \n",
    "    #1) prepare classifier,cv. max_features=1, to prevent masking \n",
    "    clf=DecisionTreeClassifier(criterion='entropy',max_features=1,class_weight='balanced',min_weight_fraction_leaf=minWLeaf) \n",
    "    clf=BaggingClassifier(base_estimator=clf,n_estimators=n_estimators,max_features=1.,max_samples=max_samples,oob_score=True,n_jobs=n_jobs) \n",
    "    fit=clf.fit(X=trnsX,y=cont['bin'],sample_weight=cont['w'].values) \n",
    "    oob=fit.oob_score_\n",
    "    if method=='MDI':\n",
    "        imp=featImpMDI(fit,featNames=trnsX.columns) \n",
    "        oos=cvScore(clf,X=trnsX,y=cont['bin'],cv=cv,sample_weight=cont['w'],\n",
    "                    t1=cont['t1'],pctEmbargo=pctEmbargo,scoring=scoring).mean() \n",
    "    elif method=='MDA':\n",
    "        imp,oos=featImpMDA(clf,X=trnsX,y=cont['bin'],cv=cv,sample_weight=cont['w'], \n",
    "                           t1=cont['t1'],pctEmbargo=pctEmbargo,scoring=scoring)\n",
    "    elif method=='SFI': \n",
    "        cvGen=PurgedKFold(n_splits=cv,t1=cont['t1'],pctEmbargo=pctEmbargo) \n",
    "        oos=cvScore(clf,X=trnsX,y=cont['bin'],sample_weight=cont['w'],scoring=scoring,\n",
    "                    cvGen=cvGen).mean()\n",
    "        clf.n_jobs=1 # paralellize auxFeatImpSFI rather than clf \n",
    "        imp=mpPandasObj(auxFeatImpSFI,('featNames',trnsX.columns),numThreads,\n",
    "                        clf=clf,trnsX=trnsX,cont=cont,scoring=scoring,cvGen=cvGen) \n",
    "    return imp,oob,oos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc583b2-48b0-4eb9-98f1-a42544e226c7",
   "metadata": {},
   "source": [
    "### SNIPPET 8.9 CALLING ALL COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8445a6e4-c722-40b1-945c-506c8451c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFunc(n_features=40,n_informative=10,n_redundant=10,n_estimators=1000, n_samples=10000,cv=10):\n",
    "    # test the performance of the feat importance functions on artificial data \n",
    "    # Nr noise features = n_features—n_informative—n_redundant \n",
    "    trnsX,cont=getTestData(n_features,n_informative,n_redundant,n_samples)\n",
    "    dict0={'minWLeaf':[0.],'scoring':['accuracy'],'method':['MDI','MDA','SFI'], 'max_samples':[1.]}\n",
    "    jobs,out=(dict(izip(dict0,i)) for i in product(*dict0.values())),[] \n",
    "    kargs={'pathOut':'./testFunc/','n_estimators':n_estimators,\n",
    "            'tag':'testFunc','cv':cv}\n",
    "    for job in jobs:\n",
    "        job['simNum']=job['method']+'_'+job['scoring']+'_'+'%.2f'%job['minWLeaf']+'_'+str(job['max_samples'])\n",
    "        print(job['simNum'])\n",
    "        kargs.update(job) \n",
    "        imp,oob,oos=featImportance(trnsX=trnsX,cont=cont,**kargs) \n",
    "        plotFeatImportance(imp=imp,oob=oob,oos=oos,**kargs) \n",
    "        df0=imp[['mean']]/imp['mean'].abs().sum() \n",
    "        df0['type']=[i[0] for i in df0.index] \n",
    "        df0=df0.groupby('type')['mean'].sum().to_dict() \n",
    "        df0.update({'oob':oob,'oos':oos});df0.update(job) \n",
    "        out.append(df0)\n",
    "    out=pd.DataFrame(out).sort_values(['method','scoring','minWLeaf','max_samples']) \n",
    "    out=out['method','scoring','minWLeaf','max_samples','I','R','N','oob','oos'] \n",
    "    out.to_csv(kargs['pathOut']+'stats.csv')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c221a69-fc8f-461b-a7d8-40e68fcb2e54",
   "metadata": {},
   "source": [
    "### SNIPPET 8.10 FEATURE IMPORTANCE PLOTTING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcdb9b2-2614-427c-933e-be43126bb1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFeatImportance(pathOut,imp,oob,oos,method,tag=0,simNum=0,**kargs): \n",
    "    '''plot mean imp bars with std'''\n",
    "    mpl.figure(figsize=(10,imp.shape[0]/5.)) \n",
    "    imp=imp.sort_values('mean',ascending=True) \n",
    "    ax=imp['mean'].plot(kind='barh',color='b',alpha=.25,xerr=imp['std'],error_kw={'ecolor':'r'})\n",
    "    if method=='MDI':\n",
    "        mpl.xlim([0,imp.sum(axis=1).max()]) \n",
    "        mpl.axvline(1./imp.shape[0],linewidth=1,color='r',linestyle='dotted')\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    for i,j in zip(ax.patches,imp.index):\n",
    "        ax.text(i.get_width()/2, i.get_y()+i.get_height()/2,j,ha='center',va='center',\n",
    "                color='black')\n",
    "    mpl.title('tag='+tag+' | simNum='+str(simNum)+' | oob='+str(round(oob,4))+\n",
    "    ' | oos='+str(round(oos,4))) \n",
    "    mpl.savefig(pathOut+'featImportance_'+str(simNum)+'.png',dpi=100) \n",
    "    mpl.clf();mpl.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2dd53-be6b-4432-93e4-84f667f85e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
